2023-02-16 14:18:38 [INFO] [task_scheduler.cc:158] Initializing Task #49: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
2023-02-16 14:18:38 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 7, 7, 32, 2048, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 7, 7, 32):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 7, 7, 32):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

2023-02-16 14:18:38 [INFO] [task_scheduler.cc:162] Total 3 design space(s) generated
2023-02-16 14:18:38 [INFO] [task_scheduler.cc:168] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 1, 1, 1, 4, 1, 7, 4, 2048, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                    oc_chunk = T.axis.spatial(16, i1_3 + i1_0 * 8 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(7, i2_2 + i2_3 + i2_0 + i2_1)
                    ow = T.axis.spatial(7, i3_3 + i3_0 * 7 + i3_1 + i3_2)
                    oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 8 + i4_2 * 4 + i4_3)
                    ic = T.axis.reduce(2048, i5_1 + i5_0)
                    kh = T.axis.reduce(1, i6_0 + i6_1)
                    kw = T.axis.reduce(1, i7_1 + i7_0)
                    T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 7, 7, 32):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2048, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
2023-02-16 14:18:38 [INFO] [task_scheduler.cc:168] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 7, 1, 1, 1, 4, 1, 7, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2048, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_3 + i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(7, i2_2 + i2_3 + i2_0 + i2_1)
                        ow = T.axis.spatial(7, i3_3 + i3_0 * 7 + i3_1 + i3_2)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 8 + i4_2 * 4 + i4_3)
                        ic = T.axis.reduce(2048, i5_1 + i5_0)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 1, 8):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1 = T.axis.spatial(7, i3_1 + ax3)
                        ax4_1 = T.axis.spatial(32, i4_1 * 8 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2048, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:18:38 [INFO] [task_scheduler.cc:168] Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 7, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 7, 4, 2048, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_3 + i1_0 * 8 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(7, i2_2 + i2_3 + i2_0 + i2_1)
                        ow = T.axis.spatial(7, i3_3 + i3_0 * 7 + i3_1 + i3_2)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 8 + i4_2 * 4 + i4_3)
                        ic = T.axis.reduce(2048, i5_1 + i5_0)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 1, 7, 32):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(7, i2_0 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 2, 1])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[7, 1, 1, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 7, 1, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 4])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[2048, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:20:39 [INFO] [task_scheduler.cc:158] Initializing Task #49: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14"
2023-02-16 14:20:39 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 7, 7, 32, 2048, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 7, 7, 32):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 7, 7, 32):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

2023-02-16 14:20:39 [INFO] [task_scheduler.cc:162] Total 3 design space(s) generated
2023-02-16 14:20:39 [INFO] [task_scheduler.cc:168] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 1, 4, 1, 1, 2, 32, 1, 1, 1, 1, 7, 7, 8, 64, 1, 1, 1, 2, 1, 1, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 2 + i1_2 * 2 + i1_3)
                    oh = T.axis.spatial(7, i2_0 * 7 + i2_1 * 7 + i2_2 + i2_3)
                    ow = T.axis.spatial(7, i3_3 + i3_0 * 7 + i3_1 * 7 + i3_2)
                    oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 16 + i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(2048, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, i6_0 + i6_1)
                    kw = T.axis.reduce(1, i7_1 + i7_0)
                    T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 7, 7, 32):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
2023-02-16 14:20:39 [INFO] [task_scheduler.cc:168] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":512, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 1, 1, 1, 1, 4, 1, 1, 2):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(32, 1, 1, 1, 1, 7, 7, 8, 64, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 2 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(7, i2_0 * 7 + i2_1 * 7 + i2_2 + i2_3)
                        ow = T.axis.spatial(7, i3_3 + i3_0 * 7 + i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 16 + i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(2048, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 7, 16):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 2 + ax1)
                        ax2_1, ax3_1 = T.axis.remap("SS", [ax2, ax3])
                        ax4_1 = T.axis.spatial(32, i4_1 * 16 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:20:39 [INFO] [task_scheduler.cc:168] Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 512, 7, 7, 4), "float32"], p1: T.Buffer[(16, 512, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 7, 7, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 7, 7, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 1, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 4, 1, 1, 2, 32, 1, 1, 1, 1, 7, 7, 8, 64, 1, 1, 1, 2, 1, 1, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 2 + i1_2 * 2 + i1_3)
                        oh = T.axis.spatial(7, i2_0 * 7 + i2_1 * 7 + i2_2 + i2_3)
                        ow = T.axis.spatial(7, i3_3 + i3_0 * 7 + i3_1 * 7 + i3_2)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 16 + i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(2048, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 7, 32):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1, ax3_1, ax4_1 = T.axis.remap("SSS", [ax2, ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 4, 1, 2])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 7, 1])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 8, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[32, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 15:24:43 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-02-16 15:24:43 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-02-16 15:24:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 15:24:44 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2023-02-16 15:24:45 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 15:24:46 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 15:24:46 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 15:24:47 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 15:24:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9999  0.9958  0.9955  0.9948  0.9945  0.9944  0.9943  0.9940  0.9937  0.9934  0.9928  0.9923  0.9918  0.9914  0.9906  0.9901
[17 : 32]:	0.9879  0.9878  0.9877  0.9871  0.9849  0.9848  0.9841  0.9840  0.9835  0.9832  0.9832  0.9825  0.9822  0.9816  0.9816  0.9813
[33 : 48]:	0.9812  0.9793  0.9792  0.9779  0.9776  0.9770  0.9761  0.9750  0.9747  0.9745  0.9740  0.9731  0.9722  0.9717  0.9717  0.9716
[49 : 64]:	0.9706  0.9704  0.9704  0.9701  0.9688  0.9685  0.9672  0.9672  0.9670  0.9668  0.9662  0.9658  0.9648  0.9643  0.9628  0.9627
2023-02-16 15:24:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-02-16 15:24:48 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #1: GFLOPs: 17.0577. Time: 6027.2095 us. Best GFLOPs: 17.0577
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #2: GFLOPs: 33.0690. Time: 3108.9771 us. Best GFLOPs: 33.0690
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #3: GFLOPs: 13.4264. Time: 7657.3713 us. Best GFLOPs: 33.0690
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #4: GFLOPs: 34.7636. Time: 2957.4189 us. Best GFLOPs: 34.7636
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #5: GFLOPs: 12.5042. Time: 8222.1036 us. Best GFLOPs: 34.7636
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #6: GFLOPs: 36.5908. Time: 2809.7429 us. Best GFLOPs: 36.5908
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #7: GFLOPs: 16.2699. Time: 6319.0691 us. Best GFLOPs: 36.5908
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #8: GFLOPs: 73.1435. Time: 1405.6009 us. Best GFLOPs: 73.1435
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #9: GFLOPs: 23.4726. Time: 4380.0340 us. Best GFLOPs: 73.1435
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #10: GFLOPs: 82.5882. Time: 1244.8583 us. Best GFLOPs: 82.5882
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #11: GFLOPs: 5.3368. Time: 19264.5236 us. Best GFLOPs: 82.5882
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #12: GFLOPs: 32.8376. Time: 3130.8820 us. Best GFLOPs: 82.5882
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #13: GFLOPs: 125.8525. Time: 816.9135 us. Best GFLOPs: 125.8525
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #14: GFLOPs: 19.7827. Time: 5197.0073 us. Best GFLOPs: 125.8525
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #15: GFLOPs: 3.5631. Time: 28854.0023 us. Best GFLOPs: 125.8525
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #16: GFLOPs: 53.6167. Time: 1917.5120 us. Best GFLOPs: 125.8525
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #17: GFLOPs: 48.8349. Time: 2105.2681 us. Best GFLOPs: 125.8525
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #18: GFLOPs: 138.6212. Time: 741.6661 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #19: GFLOPs: 28.2982. Time: 3633.1178 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #20: GFLOPs: 107.4751. Time: 956.5995 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #21: GFLOPs: 20.5252. Time: 5008.9874 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #22: GFLOPs: 18.1693. Time: 5658.4692 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #23: GFLOPs: 39.9454. Time: 2573.7789 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #24: GFLOPs: 24.6958. Time: 4163.0788 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #25: GFLOPs: 8.4554. Time: 12159.1260 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #26: GFLOPs: 1.9458. Time: 52836.8120 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #27: GFLOPs: 11.0318. Time: 9319.4775 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #28: GFLOPs: 29.1780. Time: 3523.5688 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #29: GFLOPs: 11.5654. Time: 8889.5079 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #30: GFLOPs: 28.2293. Time: 3641.9768 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #31: GFLOPs: 83.8142. Time: 1226.6495 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #32: GFLOPs: 45.8591. Time: 2241.8824 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #33: GFLOPs: 16.0212. Time: 6417.1733 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #34: GFLOPs: 74.7245. Time: 1375.8632 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #35: GFLOPs: 46.0548. Time: 2232.3551 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #36: GFLOPs: 16.9142. Time: 6078.3449 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #37: GFLOPs: 12.6835. Time: 8105.8288 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #38: GFLOPs: 9.9445. Time: 10338.4577 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #39: GFLOPs: 6.4302. Time: 15988.8072 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #40: GFLOPs: 16.4017. Time: 6268.2945 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #41: GFLOPs: 18.6906. Time: 5500.6692 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #42: GFLOPs: 33.1553. Time: 3100.8820 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #43: GFLOPs: 12.4961. Time: 8227.4247 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #44: GFLOPs: 70.4395. Time: 1459.5592 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #45: GFLOPs: 22.7326. Time: 4522.5995 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #46: GFLOPs: 40.2388. Time: 2555.0152 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #47: GFLOPs: 36.4447. Time: 2821.0020 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #48: GFLOPs: 13.6007. Time: 7559.1944 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #49: GFLOPs: 63.9347. Time: 1608.0569 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #50: GFLOPs: 16.7544. Time: 6136.3457 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #51: GFLOPs: 53.4668. Time: 1922.8868 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #52: GFLOPs: 73.3762. Time: 1401.1436 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #53: GFLOPs: 56.6743. Time: 1814.0606 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #54: GFLOPs: 41.6106. Time: 2470.7804 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #55: GFLOPs: 62.4231. Time: 1646.9975 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #56: GFLOPs: 23.3300. Time: 4406.7982 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #57: GFLOPs: 11.1015. Time: 9260.9493 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #58: GFLOPs: 119.4226. Time: 860.8974 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #59: GFLOPs: 22.5721. Time: 4554.7613 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #60: GFLOPs: 55.0457. Time: 1867.7305 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #61: GFLOPs: 30.0005. Time: 3426.9685 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #62: GFLOPs: 24.3759. Time: 4217.7099 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #63: GFLOPs: 11.9592. Time: 8596.7791 us. Best GFLOPs: 138.6212
2023-02-16 15:36:41 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #64: GFLOPs: 12.6751. Time: 8111.2283 us. Best GFLOPs: 138.6212
2023-02-16 16:01:26 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-02-16 16:01:26 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2023-02-16 16:01:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 16:01:27 [INFO] [evolutionary_search.cc:723] Sampled 448 candidate(s)
2023-02-16 16:01:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 16:01:31 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 16:01:33 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 16:01:35 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x25d42618)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f8c0828)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x280d6878)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x2ee61ef8)]: 0 failure(s)
2023-02-16 16:01:36 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9659  0.8764  0.8725  0.8627  0.8627  0.8609  0.8545  0.8426  0.8401  0.8305  0.8287  0.8279  0.8279  0.8277  0.8270  0.8264
[17 : 32]:	0.8230  0.8202  0.8169  0.8162  0.8096  0.8001  0.7977  0.7974  0.7970  0.7844  0.7810  0.7753  0.7681  0.7650  0.7611  0.7585
[33 : 48]:	0.7583  0.7554  0.7545  0.7544  0.7540  0.7540  0.7534  0.7533  0.7533  0.7507  0.7502  0.7497  0.7497  0.7495  0.7486  0.7479
[49 : 64]:	0.7419  0.7400  0.7389  0.7352  0.7334  0.7320  0.7319  0.7319  0.7304  0.7304  0.7304  0.7304  0.7303  0.7296  0.7249  0.7246
2023-02-16 16:01:36 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-02-16 16:01:36 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #65: GFLOPs: 125.7312. Time: 817.7017 us. Best GFLOPs: 138.6212
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #66: GFLOPs: 113.4633. Time: 906.1131 us. Best GFLOPs: 138.6212
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #67: GFLOPs: 107.1289. Time: 959.6908 us. Best GFLOPs: 138.6212
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #68: GFLOPs: 200.3181. Time: 513.2368 us. Best GFLOPs: 200.3181
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #69: GFLOPs: 213.3177. Time: 481.9601 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #70: GFLOPs: 105.3110. Time: 976.2574 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #71: GFLOPs: 140.7752. Time: 730.3178 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #72: GFLOPs: 197.2193. Time: 521.3009 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #73: GFLOPs: 120.6366. Time: 852.2342 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #74: GFLOPs: 88.0383. Time: 1167.7942 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #75: GFLOPs: 147.1123. Time: 698.8583 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #76: GFLOPs: 141.9223. Time: 724.4149 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #77: GFLOPs: 98.6850. Time: 1041.8062 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #78: GFLOPs: 169.9993. Time: 604.7709 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #79: GFLOPs: 98.0346. Time: 1048.7182 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #80: GFLOPs: 74.9328. Time: 1372.0385 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #81: GFLOPs: 208.4174. Time: 493.2920 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #82: GFLOPs: 96.2652. Time: 1067.9937 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #83: GFLOPs: 134.6354. Time: 763.6227 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #84: GFLOPs: 129.5684. Time: 793.4852 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #85: GFLOPs: 66.6275. Time: 1543.0659 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #86: GFLOPs: 197.7660. Time: 519.8601 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #87: GFLOPs: 141.2587. Time: 727.8182 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #88: GFLOPs: 204.2289. Time: 503.4088 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #89: GFLOPs: 158.6546. Time: 648.0156 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #90: GFLOPs: 188.0564. Time: 546.7009 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #91: GFLOPs: 128.0563. Time: 802.8547 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #92: GFLOPs: 71.9514. Time: 1428.8908 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #93: GFLOPs: 154.6779. Time: 664.6754 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #94: GFLOPs: 159.2675. Time: 645.5215 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #95: GFLOPs: 74.4784. Time: 1380.4087 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #96: GFLOPs: 134.7329. Time: 763.0700 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #97: GFLOPs: 145.6288. Time: 705.9772 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #98: GFLOPs: 67.5835. Time: 1521.2388 us. Best GFLOPs: 213.3177
2023-02-16 16:03:43 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #99: GFLOPs: 90.5439. Time: 1135.4783 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #100: GFLOPs: 113.2421. Time: 907.8835 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #101: GFLOPs: 85.6901. Time: 1199.7958 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #102: GFLOPs: 73.1737. Time: 1405.0220 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #103: GFLOPs: 89.0587. Time: 1154.4146 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #104: GFLOPs: 121.1250. Time: 848.7980 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #105: GFLOPs: 114.7973. Time: 895.5844 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #106: GFLOPs: 64.8373. Time: 1585.6714 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #107: GFLOPs: 163.6454. Time: 628.2524 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #108: GFLOPs: 149.8922. Time: 685.8969 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #109: GFLOPs: 162.4071. Time: 633.0426 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #110: GFLOPs: 91.6455. Time: 1121.8301 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #111: GFLOPs: 156.0440. Time: 658.8565 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #112: GFLOPs: 98.9968. Time: 1038.5242 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #113: GFLOPs: 81.5544. Time: 1260.6379 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #114: GFLOPs: 105.1508. Time: 977.7443 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #115: GFLOPs: 153.5122. Time: 669.7228 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #116: GFLOPs: 82.8296. Time: 1241.2311 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #117: GFLOPs: 126.1094. Time: 815.2497 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #118: GFLOPs: 113.4215. Time: 906.4478 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #119: GFLOPs: 70.5768. Time: 1456.7208 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #120: GFLOPs: 95.6469. Time: 1074.8979 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #121: GFLOPs: 108.1402. Time: 950.7161 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #122: GFLOPs: 99.8970. Time: 1029.1658 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #123: GFLOPs: 94.4316. Time: 1088.7310 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #124: GFLOPs: 109.9384. Time: 935.1658 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #125: GFLOPs: 82.6238. Time: 1244.3223 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #126: GFLOPs: 34.2989. Time: 2997.4932 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #127: GFLOPs: 43.9598. Time: 2338.7401 us. Best GFLOPs: 213.3177
2023-02-16 16:03:44 [INFO] [task_scheduler.cc:129] [Task #49: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_14] Trial #128: GFLOPs: 64.4654. Time: 1594.8182 us. Best GFLOPs: 213.3177
