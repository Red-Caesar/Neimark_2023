2023-02-16 14:18:37 [INFO] [task_scheduler.cc:158] Initializing Task #31: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1"
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 28, 28, 32, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], p3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

2023-02-16 14:18:37 [INFO] [task_scheduler.cc:162] Total 3 design space(s) generated
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:168] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 8, 4, 1, 1, 1, 1, 7, 2, 16, 2, 1, 1, 1, 2, 1, 1, 1, 64, 1, 1, 1, 1, 1, 14, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                    oc_chunk = T.axis.spatial(16, i1_3 + i1_0 * 2 + i1_1 * 2 + i1_2)
                    oh = T.axis.spatial(28, i2_0 * 7 + i2_1 + i2_2 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 28 + i3_1 * 14 + i3_2 * 14 + i3_3)
                    oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 2 + i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, i6_0 + i6_1)
                    kw = T.axis.reduce(1, i7_1 + i7_0)
                    T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, ax2, ax3, ax4], p3[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15], preserve_unit_iters=True)
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23], preserve_unit_iters=True)
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31], preserve_unit_iters=True)
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39], preserve_unit_iters=True)
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47], preserve_unit_iters=True)
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53], preserve_unit_iters=True)
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57], preserve_unit_iters=True)
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61], preserve_unit_iters=True)
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:168] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 8, 4, 1, 1, 1, 1, 7, 2, 16):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(2, 1, 1, 1, 2, 1, 1, 1, 64, 1, 1, 1, 1, 1, 14, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_3 + i1_0 * 2 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_1 + i2_2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 28 + i3_1 * 14 + i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 2 + i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 1, 14, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + i2_1 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 14 + ax3)
                        ax4_1 = T.axis.spatial(32, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p3[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15], preserve_unit_iters=True)
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23], preserve_unit_iters=True)
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31], preserve_unit_iters=True)
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39], preserve_unit_iters=True)
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47], preserve_unit_iters=True)
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53], preserve_unit_iters=True)
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57], preserve_unit_iters=True)
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61], preserve_unit_iters=True)
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:168] Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 8, 4, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 7, 2, 16, 2, 1, 1, 1, 2, 1, 1, 1, 64, 1, 1, 1, 1, 1, 14, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_3 + i1_0 * 2 + i1_1 * 2 + i1_2)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_1 + i2_2 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 28 + i3_1 * 14 + i3_2 * 14 + i3_3)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 2 + i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 2, 7, 28, 32):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 2 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p3[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15], preserve_unit_iters=True)
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[8, 1, 2, 1])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23], preserve_unit_iters=True)
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 7, 1, 1])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31], preserve_unit_iters=True)
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 2, 1, 14])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39], preserve_unit_iters=True)
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 16, 1, 2])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47], preserve_unit_iters=True)
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[2, 64])
l54, l55 = sch.split(loop=l9, factors=[v52, v53], preserve_unit_iters=True)
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57], preserve_unit_iters=True)
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61], preserve_unit_iters=True)
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
2023-02-16 14:20:38 [INFO] [task_scheduler.cc:158] Initializing Task #31: "fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1"
2023-02-16 14:20:38 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
        T_add_1 = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 28, 28, 32, 128, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, ax2, ax3, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
            with T.block("T_add_1"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4], p3[ax0, ax1, 0, 0, ax4])
                T.writes(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T_add_1[ax0, ax1, ax2, ax3, ax4] = T_add[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add_1[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add_1[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

2023-02-16 14:20:38 [INFO] [task_scheduler.cc:162] Total 3 design space(s) generated
2023-02-16 14:20:38 [INFO] [task_scheduler.cc:168] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 4, 1, 1, 1, 1, 1, 7, 1, 64, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 4, 7, 2, 16):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 8 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(28, i2_0 * 7 + i2_1 * 7 + i2_2 * 7 + i2_3)
                    ow = T.axis.spatial(28, i3_0 * 28 + i3_1 * 4 + i3_2 * 2 + i3_3)
                    oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 32 + i4_2 * 16 + i4_3)
                    ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                    kh = T.axis.reduce(1, i6_0 + i6_1)
                    kw = T.axis.reduce(1, i7_1 + i7_0)
                    T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 28, 28, 32):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, ax2, ax3, ax4], p3[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, ax2, ax3, ax4] + p3[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15], preserve_unit_iters=True)
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23], preserve_unit_iters=True)
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31], preserve_unit_iters=True)
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39], preserve_unit_iters=True)
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 16])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47], preserve_unit_iters=True)
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53], preserve_unit_iters=True)
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57], preserve_unit_iters=True)
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61], preserve_unit_iters=True)
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:20:38 [INFO] [task_scheduler.cc:168] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 4, 1, 1, 1, 1, 1, 7, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(64, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 4, 7, 2, 16):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_1 * 7 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 28 + i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 32 + i4_2 * 16 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 4, 32):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1 = T.axis.spatial(28, i3_1 * 4 + ax3)
                        ax4_1 = T.axis.spatial(32, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p3[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15], preserve_unit_iters=True)
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23], preserve_unit_iters=True)
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31], preserve_unit_iters=True)
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39], preserve_unit_iters=True)
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 16])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47], preserve_unit_iters=True)
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53], preserve_unit_iters=True)
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57], preserve_unit_iters=True)
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61], preserve_unit_iters=True)
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l49, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
2023-02-16 14:20:38 [INFO] [task_scheduler.cc:168] Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 28, 28, 4), "float32"], p1: T.Buffer[(16, 32, 1, 1, 4, 32), "float32"], p2: T.Buffer[(1, 16, 28, 28, 32), "float32"], p3: T.Buffer[(1, 16, 1, 1, 32), "float32"], T_relu: T.Buffer[(1, 16, 28, 28, 32), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 28, 28, 32], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 4, 1, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 1, 7, 1, 64, 1, 1, 1, 2, 1, 2, 2, 2, 1, 1, 1, 4, 7, 2, 16):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 8 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(28, i2_0 * 7 + i2_1 * 7 + i2_2 * 7 + i2_3)
                        ow = T.axis.spatial(28, i3_0 * 28 + i3_1 * 4 + i3_2 * 2 + i3_3)
                        oc_block = T.axis.spatial(32, i4_0 * 32 + i4_1 * 32 + i4_2 * 16 + i4_3)
                        ic = T.axis.reduce(128, i5_0 * 2 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 4, oh + kh, ow + kw, ic % 4], p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 4, oh + kh, ow + kw, ic % 4] * p1[oc_chunk, ic // 4, kh, kw, ic % 4, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 7, 28, 32):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(28, i2_0 * 7 + ax2)
                        ax3_1, ax4_1 = T.axis.remap("SS", [ax3, ax4])
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p3[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p3[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="T_add_1", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b2)
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l4, l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b0)
v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l16, l17, l18, l19 = sch.split(loop=l4, factors=[v12, v13, v14, v15], preserve_unit_iters=True)
v20, v21, v22, v23 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 1, 2, 4])
l24, l25, l26, l27 = sch.split(loop=l5, factors=[v20, v21, v22, v23], preserve_unit_iters=True)
v28, v29, v30, v31 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 1, 1, 7])
l32, l33, l34, l35 = sch.split(loop=l6, factors=[v28, v29, v30, v31], preserve_unit_iters=True)
v36, v37, v38, v39 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 7, 2, 2])
l40, l41, l42, l43 = sch.split(loop=l7, factors=[v36, v37, v38, v39], preserve_unit_iters=True)
v44, v45, v46, v47 = sch.sample_perfect_tile(loop=l8, n=4, max_innermost_factor=64, decision=[1, 1, 2, 16])
l48, l49, l50, l51 = sch.split(loop=l8, factors=[v44, v45, v46, v47], preserve_unit_iters=True)
v52, v53 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[64, 2])
l54, l55 = sch.split(loop=l9, factors=[v52, v53], preserve_unit_iters=True)
v56, v57 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l58, l59 = sch.split(loop=l10, factors=[v56, v57], preserve_unit_iters=True)
v60, v61 = sch.sample_perfect_tile(loop=l11, n=2, max_innermost_factor=64, decision=[1, 1])
l62, l63 = sch.split(loop=l11, factors=[v60, v61], preserve_unit_iters=True)
sch.reorder(l16, l24, l32, l40, l48, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50, l55, l59, l63, l19, l27, l35, l43, l51)
b64, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b64, loop=l48, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.vectorize", ann_val=64)
v65 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v65)
2023-02-16 15:00:40 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-02-16 15:00:40 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-02-16 15:00:40 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x1f3d8068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f9d4078)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x220dfe18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x1f70b8d8)]: 0 failure(s)
2023-02-16 15:00:40 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2023-02-16 15:00:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x1f3d8068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f9d4078)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x220dfe18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x1f70b8d8)]: 0 failure(s)
2023-02-16 15:00:42 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x1f3d8068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f9d4078)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x220dfe18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x1f70b8d8)]: 0 failure(s)
2023-02-16 15:00:43 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x1f3d8068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f9d4078)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x220dfe18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x1f70b8d8)]: 0 failure(s)
2023-02-16 15:00:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x1f3d8068)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x1f9d4078)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x220dfe18)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x1f70b8d8)]: 0 failure(s)
2023-02-16 15:00:44 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0000  0.9993  0.9989  0.9976  0.9969  0.9969  0.9969  0.9959  0.9957  0.9957  0.9957  0.9956  0.9950  0.9947  0.9940  0.9936
[17 : 32]:	0.9934  0.9933  0.9932  0.9929  0.9928  0.9927  0.9924  0.9923  0.9921  0.9918  0.9915  0.9913  0.9909  0.9906  0.9906  0.9905
[33 : 48]:	0.9881  0.9859  0.9856  0.9845  0.9845  0.9844  0.9839  0.9832  0.9829  0.9794  0.9793  0.9793  0.9780  0.9768  0.9761  0.9758
[49 : 64]:	0.9746  0.9744  0.9736  0.9733  0.9729  0.9728  0.9719  0.9718  0.9716  0.9715  0.9712  0.9709  0.9709  0.9707  0.9706  0.9701
2023-02-16 15:00:44 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-02-16 15:00:44 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #1: GFLOPs: 6.7605. Time: 15378.3378 us. Best GFLOPs: 6.7605
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #2: GFLOPs: 24.8706. Time: 4180.2236 us. Best GFLOPs: 24.8706
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #3: GFLOPs: 59.4875. Time: 1747.6728 us. Best GFLOPs: 59.4875
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #4: GFLOPs: 58.7813. Time: 1768.6701 us. Best GFLOPs: 59.4875
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #5: GFLOPs: 19.5890. Time: 5307.2916 us. Best GFLOPs: 59.4875
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #6: GFLOPs: 41.3370. Time: 2515.0536 us. Best GFLOPs: 59.4875
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #7: GFLOPs: 28.9486. Time: 3591.3535 us. Best GFLOPs: 59.4875
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #8: GFLOPs: 24.6849. Time: 4211.6727 us. Best GFLOPs: 59.4875
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #9: GFLOPs: 136.6832. Time: 760.6251 us. Best GFLOPs: 136.6832
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #10: GFLOPs: 10.1614. Time: 10231.2895 us. Best GFLOPs: 136.6832
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #11: GFLOPs: 19.9931. Time: 5200.0195 us. Best GFLOPs: 136.6832
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #12: GFLOPs: 171.3118. Time: 606.8740 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #13: GFLOPs: 48.0896. Time: 2161.8950 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #14: GFLOPs: 15.6608. Time: 6638.5128 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #15: GFLOPs: 34.4683. Time: 3016.2422 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #16: GFLOPs: 127.8263. Time: 813.3275 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #17: GFLOPs: 23.9309. Time: 4344.3733 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #18: GFLOPs: 89.7948. Time: 1157.8032 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #19: GFLOPs: 39.5315. Time: 2629.9213 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #20: GFLOPs: 13.2284. Time: 7859.2017 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #21: GFLOPs: 26.8956. Time: 3865.4915 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #22: GFLOPs: 96.8565. Time: 1073.3886 us. Best GFLOPs: 171.3118
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #23: GFLOPs: 192.0512. Time: 541.3384 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #24: GFLOPs: 16.2826. Time: 6385.0001 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #25: GFLOPs: 98.1397. Time: 1059.3541 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #26: GFLOPs: 21.3368. Time: 4872.5501 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #27: GFLOPs: 111.7078. Time: 930.6839 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #28: GFLOPs: 149.4699. Time: 695.5560 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #29: GFLOPs: 33.4776. Time: 3105.4955 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #30: GFLOPs: 33.7493. Time: 3080.5031 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #31: GFLOPs: 28.4845. Time: 3649.8737 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #32: GFLOPs: 32.3780. Time: 3210.9656 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #33: GFLOPs: 52.1279. Time: 1994.4153 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #34: GFLOPs: 81.3061. Time: 1278.6816 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #35: GFLOPs: 13.7462. Time: 7563.1687 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #36: GFLOPs: 52.0744. Time: 1996.4657 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #37: GFLOPs: 69.3645. Time: 1498.8169 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #38: GFLOPs: 60.3007. Time: 1724.1036 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #39: GFLOPs: 7.7544. Time: 13407.2103 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #40: GFLOPs: 25.7304. Time: 4040.5417 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #41: GFLOPs: 13.8438. Time: 7509.8178 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #42: GFLOPs: 52.6909. Time: 1973.1036 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #43: GFLOPs: 40.1391. Time: 2590.1124 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #44: GFLOPs: 35.9910. Time: 2888.6277 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #45: GFLOPs: 32.0043. Time: 3248.4589 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #46: GFLOPs: 14.5910. Time: 7125.2768 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #47: GFLOPs: 100.6164. Time: 1033.2780 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #48: GFLOPs: 177.9626. Time: 584.1941 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #49: GFLOPs: 19.3712. Time: 5366.9776 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #50: GFLOPs: 71.7207. Time: 1449.5769 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #51: GFLOPs: 21.1936. Time: 4905.4730 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #52: GFLOPs: 16.4842. Time: 6306.9459 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #53: GFLOPs: 156.2169. Time: 665.5148 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #54: GFLOPs: 48.5977. Time: 2139.2904 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #55: GFLOPs: 15.2729. Time: 6807.1193 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #56: GFLOPs: 51.2363. Time: 2029.1202 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #57: GFLOPs: 38.8879. Time: 2673.4428 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #58: GFLOPs: 33.8748. Time: 3069.0829 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #59: GFLOPs: 10.5446. Time: 9859.4887 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #60: GFLOPs: 129.2844. Time: 804.1548 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #61: GFLOPs: 96.9047. Time: 1072.8553 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #62: GFLOPs: 14.4501. Time: 7194.7184 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #63: GFLOPs: 66.8975. Time: 1554.0888 us. Best GFLOPs: 192.0512
2023-02-16 15:36:32 [INFO] [task_scheduler.cc:129] [Task #31: fused_nn_contrib_conv2d_NCHWc_add_add_nn_relu_1] Trial #64: GFLOPs: 33.1760. Time: 3133.7336 us. Best GFLOPs: 192.0512
