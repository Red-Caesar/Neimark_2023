2023-02-16 14:18:37 [INFO] [task_scheduler.cc:158] Initializing Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 56, 56, 8, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 8):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 8):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

2023-02-16 14:18:37 [INFO] [task_scheduler.cc:162] Total 3 design space(s) generated
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:168] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 1, 2, 4, 1, 1, 2, 7, 2, 4, 256, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 8, 1, 7, 1):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                    oc_chunk = T.axis.spatial(16, i1_0 * 16 + i1_1 * 8 + i1_2 * 8 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 4 + i2_2 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 14 + i3_1 * 7 + i3_2 * 7 + i3_3)
                    oc_block = T.axis.spatial(8, i4_0 * 8 + i4_1 * 2 + i4_2 + i4_3)
                    ic = T.axis.reduce(256, i5_1 + i5_0)
                    kh = T.axis.reduce(1, i6_0 + i6_1)
                    kw = T.axis.reduce(1, i7_1 + i7_0)
                    T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 8):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:168] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 1, 2, 4, 1, 1, 2, 7, 2, 4):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(256, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 8, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 16 + i1_1 * 8 + i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 4 + i2_2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 14 + i3_1 * 7 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(8, i4_0 * 8 + i4_1 * 2 + i4_2 + i4_3)
                        ic = T.axis.reduce(256, i5_1 + i5_0)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 4, 7, 2):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_1 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + i2_1 * 4 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 14 + i3_1 * 7 + ax3)
                        ax4_1 = T.axis.spatial(8, i4_1 * 2 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:18:37 [INFO] [task_scheduler.cc:168] Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 1, 2, 4, 1):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 7, 2, 4, 256, 1, 1, 1, 1, 4, 1, 2, 1, 1, 1, 1, 8, 1, 7, 1):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 16 + i1_1 * 8 + i1_2 * 8 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 28 + i2_1 * 4 + i2_2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 14 + i3_1 * 7 + i3_2 * 7 + i3_3)
                        oc_block = T.axis.spatial(8, i4_0 * 8 + i4_1 * 2 + i4_2 + i4_3)
                        ic = T.axis.reduce(256, i5_1 + i5_0)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 16, 28, 14, 8):
                    with T.block("T_relu"):
                        ax0_1, ax1_1 = T.axis.remap("SS", [ax0, ax1])
                        ax2_1 = T.axis.spatial(56, i2_0 * 28 + ax2)
                        ax3_1 = T.axis.spatial(56, i3_0 * 14 + ax3)
                        ax4_1 = T.axis.spatial(8, ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[1, 2, 1, 8])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[2, 7, 4, 1])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[4, 2, 1, 7])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[1, 4, 2, 1])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[256, 1])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:20:37 [INFO] [task_scheduler.cc:158] Initializing Task #22: "fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4"
2023-02-16 14:20:37 [INFO] [task_scheduler.cc:35] 
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        # with T.block("root")
        conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
        T_add = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
        for i0, i1, i2, i3, i4, i5, i6, i7 in T.grid(1, 16, 56, 56, 8, 256, 1, 1):
            with T.block("conv2d_NCHWc"):
                n, oc_chunk, oh, ow, oc_block, ic, kh, kw = T.axis.remap("SSSSSRRR", [i0, i1, i2, i3, i4, i5, i6, i7])
                T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                with T.init():
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 8):
            with T.block("T_add"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                T.writes(T_add[ax0, ax1, ax2, ax3, ax4])
                T_add[ax0, ax1, ax2, ax3, ax4] = conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4]
        for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 8):
            with T.block("T_relu"):
                ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                T.reads(T_add[ax0, ax1, ax2, ax3, ax4])
                T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(T_add[ax0, ax1, ax2, ax3, ax4], T.float32(0))
    

2023-02-16 14:20:37 [INFO] [task_scheduler.cc:162] Total 3 design space(s) generated
2023-02-16 14:20:37 [INFO] [task_scheduler.cc:168] Design space #0:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":16, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 4, 1, 2, 1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 7, 1, 2, 64, 1, 1, 1, 4, 2, 56, 2):
                with T.block("conv2d_NCHWc"):
                    n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                    oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 4 + i1_3)
                    oh = T.axis.spatial(56, i2_0 * 14 + i2_1 * 14 + i2_2 * 2 + i2_3)
                    ow = T.axis.spatial(56, i3_0 * 56 + i3_1 * 56 + i3_2 * 56 + i3_3)
                    oc_block = T.axis.spatial(8, i4_0 * 4 + i4_1 * 4 + i4_2 * 2 + i4_3)
                    ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                    kh = T.axis.reduce(1, i6_0 + i6_1)
                    kw = T.axis.reduce(1, i7_1 + i7_0)
                    T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                    T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                    T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                    with T.init():
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                    conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
            for i0, i1, i2, i3, i4 in T.grid(1, 16, 56, 56, 8):
                with T.block("T_relu"):
                    ax0, ax1, ax2, ax3, ax4 = T.axis.remap("SSSSS", [i0, i1, i2, i3, i4])
                    T.reads(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4], p2[ax0, ax1, 0, 0, ax4])
                    T.writes(T_relu[ax0, ax1, ax2, ax3, ax4])
                    T_relu[ax0, ax1, ax2, ax3, ax4] = T.max(conv2d_NCHWc[ax0, ax1, ax2, ax3, ax4] + p2[ax0, ax1, 0, 0, ax4], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v63 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v63)
2023-02-16 14:20:37 [INFO] [task_scheduler.cc:168] Design space #1:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":64, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0, i0_1, i1_1, i2_1, i3_1, i4_1 in T.grid(1, 2, 4, 1, 2, 1, 2, 1, 1, 1):
                for i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(4, 1, 1, 1, 1, 7, 1, 2, 64, 1, 1, 1, 4, 2, 56, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 14 + i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 56 + i3_1 * 56 + i3_2 * 56 + i3_3)
                        oc_block = T.axis.spatial(8, i4_0 * 4 + i4_1 * 4 + i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 4, 14, 56, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4_1 = T.axis.spatial(8, i4_0 * 4 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l48, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:20:37 [INFO] [task_scheduler.cc:168] Design space #2:
# from tvm.script import tir as T
@tvm.script.ir_module
class Module:
    @T.prim_func
    def main(p0: T.Buffer[(1, 32, 56, 56, 8), "float32"], p1: T.Buffer[(16, 32, 1, 1, 8, 8), "float32"], p2: T.Buffer[(1, 16, 1, 1, 8), "float32"], T_relu: T.Buffer[(1, 16, 56, 56, 8), "float32"]):
        # function attr dict
        T.func_attr({"global_symbol": "main", "tir.noalias": True})
        # body
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.parallel":96, "meta_schedule.unroll_explicit":0, "meta_schedule.vectorize":64})
            conv2d_NCHWc = T.alloc_buffer([1, 16, 56, 56, 8], dtype="float32")
            for i0_0, i1_0, i2_0, i3_0, i4_0 in T.grid(1, 2, 4, 1, 2):
                for i0_1, i1_1, i2_1, i3_1, i4_1, i5_0, i6_0, i7_0, i0_2, i1_2, i2_2, i3_2, i4_2, i5_1, i6_1, i7_1, i0_3, i1_3, i2_3, i3_3, i4_3 in T.grid(1, 2, 1, 1, 1, 4, 1, 1, 1, 1, 7, 1, 2, 64, 1, 1, 1, 4, 2, 56, 2):
                    with T.block("conv2d_NCHWc"):
                        n = T.axis.spatial(1, i0_0 + i0_1 + i0_2 + i0_3)
                        oc_chunk = T.axis.spatial(16, i1_0 * 8 + i1_1 * 4 + i1_2 * 4 + i1_3)
                        oh = T.axis.spatial(56, i2_0 * 14 + i2_1 * 14 + i2_2 * 2 + i2_3)
                        ow = T.axis.spatial(56, i3_0 * 56 + i3_1 * 56 + i3_2 * 56 + i3_3)
                        oc_block = T.axis.spatial(8, i4_0 * 4 + i4_1 * 4 + i4_2 * 2 + i4_3)
                        ic = T.axis.reduce(256, i5_0 * 64 + i5_1)
                        kh = T.axis.reduce(1, i6_0 + i6_1)
                        kw = T.axis.reduce(1, i7_1 + i7_0)
                        T.reads(p0[n, ic // 8, oh + kh, ow + kw, ic % 8], p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block])
                        T.writes(conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block])
                        T.block_attr({"meta_schedule.tiling_structure":"SSRSRS"})
                        with T.init():
                            conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = T.float32(0)
                        conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] = conv2d_NCHWc[n, oc_chunk, oh, ow, oc_block] + p0[n, ic // 8, oh + kh, ow + kw, ic % 8] * p1[oc_chunk, ic // 8, kh, kw, ic % 8, oc_block]
                for ax0, ax1, ax2, ax3, ax4 in T.grid(1, 8, 14, 56, 4):
                    with T.block("T_relu"):
                        ax0_1 = T.axis.spatial(1, ax0)
                        ax1_1 = T.axis.spatial(16, i1_0 * 8 + ax1)
                        ax2_1 = T.axis.spatial(56, i2_0 * 14 + ax2)
                        ax3_1 = T.axis.spatial(56, ax3)
                        ax4_1 = T.axis.spatial(8, i4_0 * 4 + ax4)
                        T.reads(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1], p2[ax0_1, ax1_1, 0, 0, ax4_1])
                        T.writes(T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1])
                        T_relu[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] = T.max(conv2d_NCHWc[ax0_1, ax1_1, ax2_1, ax3_1, ax4_1] + p2[ax0_1, ax1_1, 0, 0, ax4_1], T.float32(0))
    

b0 = sch.get_block(name="conv2d_NCHWc", func_name="main")
b1 = sch.get_block(name="T_add", func_name="main")
b2 = sch.get_block(name="root", func_name="main")
sch.compute_inline(block=b1)
sch.annotate(block_or_loop=b0, ann_key="meta_schedule.tiling_structure", ann_val="SSRSRS")
l3, l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b0)
v11, v12, v13, v14 = sch.sample_perfect_tile(loop=l3, n=4, max_innermost_factor=64, decision=[1, 1, 1, 1])
l15, l16, l17, l18 = sch.split(loop=l3, factors=[v11, v12, v13, v14], preserve_unit_iters=True)
v19, v20, v21, v22 = sch.sample_perfect_tile(loop=l4, n=4, max_innermost_factor=64, decision=[2, 2, 1, 4])
l23, l24, l25, l26 = sch.split(loop=l4, factors=[v19, v20, v21, v22], preserve_unit_iters=True)
v27, v28, v29, v30 = sch.sample_perfect_tile(loop=l5, n=4, max_innermost_factor=64, decision=[4, 1, 7, 2])
l31, l32, l33, l34 = sch.split(loop=l5, factors=[v27, v28, v29, v30], preserve_unit_iters=True)
v35, v36, v37, v38 = sch.sample_perfect_tile(loop=l6, n=4, max_innermost_factor=64, decision=[1, 1, 1, 56])
l39, l40, l41, l42 = sch.split(loop=l6, factors=[v35, v36, v37, v38], preserve_unit_iters=True)
v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l7, n=4, max_innermost_factor=64, decision=[2, 1, 2, 2])
l47, l48, l49, l50 = sch.split(loop=l7, factors=[v43, v44, v45, v46], preserve_unit_iters=True)
v51, v52 = sch.sample_perfect_tile(loop=l8, n=2, max_innermost_factor=64, decision=[4, 64])
l53, l54 = sch.split(loop=l8, factors=[v51, v52], preserve_unit_iters=True)
v55, v56 = sch.sample_perfect_tile(loop=l9, n=2, max_innermost_factor=64, decision=[1, 1])
l57, l58 = sch.split(loop=l9, factors=[v55, v56], preserve_unit_iters=True)
v59, v60 = sch.sample_perfect_tile(loop=l10, n=2, max_innermost_factor=64, decision=[1, 1])
l61, l62 = sch.split(loop=l10, factors=[v59, v60], preserve_unit_iters=True)
sch.reorder(l15, l23, l31, l39, l47, l16, l24, l32, l40, l48, l53, l57, l61, l17, l25, l33, l41, l49, l54, l58, l62, l18, l26, l34, l42, l50)
b63, = sch.get_consumers(block=b0)
sch.reverse_compute_at(block=b63, loop=l47, preserve_unit_loops=True, index=-1)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.parallel", ann_val=96)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.vectorize", ann_val=64)
v64 = sch.sample_categorical(candidates=[0, 16, 64, 512], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b2, ann_key="meta_schedule.unroll_explicit", ann_val=v64)
2023-02-16 14:47:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-02-16 14:47:50 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-02-16 14:47:51 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 14:47:51 [INFO] [evolutionary_search.cc:723] Sampled 512 candidate(s)
2023-02-16 14:47:52 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 14:47:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 14:47:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 14:47:54 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 14:47:54 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9998  0.9997  0.9994  0.9980  0.9978  0.9969  0.9964  0.9963  0.9956  0.9954  0.9953  0.9952  0.9952  0.9949  0.9943  0.9940
[17 : 32]:	0.9938  0.9930  0.9923  0.9922  0.9922  0.9917  0.9916  0.9914  0.9913  0.9910  0.9902  0.9898  0.9897  0.9889  0.9883  0.9881
[33 : 48]:	0.9880  0.9873  0.9866  0.9864  0.9857  0.9855  0.9852  0.9849  0.9845  0.9841  0.9841  0.9836  0.9833  0.9824  0.9823  0.9821
[49 : 64]:	0.9819  0.9810  0.9805  0.9802  0.9794  0.9792  0.9791  0.9787  0.9781  0.9774  0.9766  0.9764  0.9762  0.9760  0.9759  0.9748
2023-02-16 14:47:54 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-02-16 14:47:54 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #1: GFLOPs: 16.5827. Time: 12442.1281 us. Best GFLOPs: 16.5827
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #2: GFLOPs: 35.7189. Time: 5776.3230 us. Best GFLOPs: 35.7189
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #3: GFLOPs: 21.7782. Time: 9473.8730 us. Best GFLOPs: 35.7189
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #4: GFLOPs: 32.4015. Time: 6367.7249 us. Best GFLOPs: 35.7189
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #5: GFLOPs: 30.1299. Time: 6847.8039 us. Best GFLOPs: 35.7189
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #6: GFLOPs: 10.9734. Time: 18802.2533 us. Best GFLOPs: 35.7189
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #7: GFLOPs: 31.2198. Time: 6608.7408 us. Best GFLOPs: 35.7189
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #8: GFLOPs: 148.0781. Time: 1393.3442 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #9: GFLOPs: 19.9288. Time: 10353.0524 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #10: GFLOPs: 11.2925. Time: 18270.9239 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #11: GFLOPs: 16.9306. Time: 12186.4070 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #12: GFLOPs: 16.3164. Time: 12645.1588 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #13: GFLOPs: 56.2024. Time: 3671.0805 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #14: GFLOPs: 27.1417. Time: 7601.7103 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #15: GFLOPs: 23.8228. Time: 8660.7544 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #16: GFLOPs: 54.4581. Time: 3788.6704 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #17: GFLOPs: 16.8262. Time: 12262.0403 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #18: GFLOPs: 22.6745. Time: 9099.3893 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #19: GFLOPs: 11.7416. Time: 17571.9924 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #20: GFLOPs: 14.7286. Time: 14008.3696 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #21: GFLOPs: 29.5421. Time: 6984.0551 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #22: GFLOPs: 18.9271. Time: 10900.9961 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #23: GFLOPs: 34.3525. Time: 6006.0745 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #24: GFLOPs: 49.5981. Time: 4159.9113 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #25: GFLOPs: 41.8763. Time: 4926.9837 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #26: GFLOPs: 128.3017. Time: 1608.1135 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #27: GFLOPs: 69.1118. Time: 2985.3595 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #28: GFLOPs: 140.1056. Time: 1472.6299 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #29: GFLOPs: 46.8699. Time: 4402.0525 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #30: GFLOPs: 79.3808. Time: 2599.1650 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #31: GFLOPs: 60.8822. Time: 3388.8986 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #32: GFLOPs: 24.3714. Time: 8465.8109 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #33: GFLOPs: 12.9312. Time: 15955.4818 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #34: GFLOPs: 32.7366. Time: 6302.5449 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #35: GFLOPs: 13.6771. Time: 15085.3319 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #36: GFLOPs: 28.2160. Time: 7312.2989 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #37: GFLOPs: 103.0893. Time: 2001.4077 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #38: GFLOPs: 21.8940. Time: 9423.7377 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #39: GFLOPs: 10.4345. Time: 19773.1376 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #40: GFLOPs: 50.8655. Time: 4056.2601 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #41: GFLOPs: 19.1984. Time: 10746.9215 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #42: GFLOPs: 48.4285. Time: 4260.3744 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #43: GFLOPs: 24.8193. Time: 8313.0493 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #44: GFLOPs: 15.8965. Time: 12979.2021 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #45: GFLOPs: 55.4760. Time: 3719.1518 us. Best GFLOPs: 148.0781
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #46: GFLOPs: 163.1389. Time: 1264.7117 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #47: GFLOPs: 3.6585. Time: 56395.6410 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #48: GFLOPs: 23.6236. Time: 8733.7896 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #49: GFLOPs: 11.7668. Time: 17534.3503 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #50: GFLOPs: 20.5416. Time: 10044.1743 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #51: GFLOPs: 42.4131. Time: 4864.6242 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #52: GFLOPs: 29.2325. Time: 7058.0217 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #53: GFLOPs: 14.9533. Time: 13797.8790 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #54: GFLOPs: 58.3414. Time: 3536.4905 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #55: GFLOPs: 40.5993. Time: 5081.9560 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #56: GFLOPs: 35.9415. Time: 5740.5476 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #57: GFLOPs: 18.1947. Time: 11339.7637 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #58: GFLOPs: 41.2487. Time: 5001.9492 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #59: GFLOPs: 32.3493. Time: 6377.9890 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #60: GFLOPs: 7.9148. Time: 26068.0651 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #61: GFLOPs: 20.5704. Time: 10030.1370 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #62: GFLOPs: 21.6484. Time: 9530.6572 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #63: GFLOPs: 40.7219. Time: 5066.6583 us. Best GFLOPs: 163.1389
2023-02-16 15:36:25 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #64: GFLOPs: 62.5294. Time: 3299.6293 us. Best GFLOPs: 163.1389
2023-02-16 16:16:26 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-02-16 16:16:26 [INFO] [evolutionary_search.cc:715] Picked top 64 candidate(s) from database
2023-02-16 16:16:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 16:16:26 [INFO] [evolutionary_search.cc:723] Sampled 448 candidate(s)
2023-02-16 16:16:28 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 16:16:30 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 16:16:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 16:16:35 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x2f25c418)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteParallelVectorizeUnroll(0x35f632c8)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteReductionBlock(0x33b1f678)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteLayout(0x219030b8)]: 0 failure(s)
2023-02-16 16:16:36 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0090  0.9916  0.9916  0.9868  0.9771  0.9696  0.9667  0.9515  0.9515  0.9495  0.9468  0.9457  0.9401  0.9401  0.9200  0.9124
[17 : 32]:	0.9048  0.8994  0.8987  0.8960  0.8812  0.8745  0.8687  0.8646  0.8620  0.8583  0.8546  0.8540  0.8540  0.8505  0.8487  0.8470
[33 : 48]:	0.8470  0.8469  0.8452  0.8442  0.8442  0.8437  0.8415  0.8374  0.8353  0.8343  0.8330  0.8321  0.8309  0.8287  0.8281  0.8274
[49 : 64]:	0.8259  0.8245  0.8244  0.8227  0.8214  0.8183  0.8166  0.8110  0.8108  0.8107  0.8099  0.8087  0.8082  0.8078  0.8051  0.8045
2023-02-16 16:16:36 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-02-16 16:16:36 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #65: GFLOPs: 102.8285. Time: 2006.4839 us. Best GFLOPs: 163.1389
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #66: GFLOPs: 139.0195. Time: 1484.1355 us. Best GFLOPs: 163.1389
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #67: GFLOPs: 192.0279. Time: 1074.4464 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #68: GFLOPs: 135.3184. Time: 1524.7273 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #69: GFLOPs: 107.5787. Time: 1917.8865 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #70: GFLOPs: 99.6167. Time: 2071.1762 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #71: GFLOPs: 105.4235. Time: 1957.0936 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #72: GFLOPs: 87.7281. Time: 2351.8544 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #73: GFLOPs: 97.3027. Time: 2120.4315 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #74: GFLOPs: 75.7364. Time: 2724.2331 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #75: GFLOPs: 77.1890. Time: 2672.9692 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #76: GFLOPs: 96.3157. Time: 2142.1602 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #77: GFLOPs: 95.5706. Time: 2158.8612 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #78: GFLOPs: 104.4211. Time: 1975.8809 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #79: GFLOPs: 90.6479. Time: 2276.1014 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #80: GFLOPs: 144.9745. Time: 1423.1727 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #81: GFLOPs: 117.2184. Time: 1760.1648 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #82: GFLOPs: 126.9032. Time: 1625.8359 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #83: GFLOPs: 65.2024. Time: 3164.3557 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #84: GFLOPs: 123.7608. Time: 1667.1174 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #85: GFLOPs: 151.8676. Time: 1358.5766 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #86: GFLOPs: 125.9760. Time: 1637.8020 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #87: GFLOPs: 149.7699. Time: 1377.6045 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #88: GFLOPs: 152.4263. Time: 1353.5968 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #89: GFLOPs: 148.4885. Time: 1389.4931 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #90: GFLOPs: 127.7267. Time: 1615.3528 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #91: GFLOPs: 138.8849. Time: 1485.5732 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #92: GFLOPs: 149.6288. Time: 1378.9042 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #93: GFLOPs: 156.7683. Time: 1316.1058 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #94: GFLOPs: 127.3566. Time: 1620.0470 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #95: GFLOPs: 144.5616. Time: 1427.2369 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #96: GFLOPs: 59.9227. Time: 3443.1650 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #97: GFLOPs: 92.6254. Time: 2227.5076 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #98: GFLOPs: 138.9387. Time: 1484.9980 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #99: GFLOPs: 175.8852. Time: 1173.0589 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #100: GFLOPs: 151.2895. Time: 1363.7677 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #101: GFLOPs: 150.5733. Time: 1370.2541 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #102: GFLOPs: 91.2637. Time: 2260.7418 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #103: GFLOPs: 74.7875. Time: 2758.7994 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #104: GFLOPs: 87.5518. Time: 2356.5899 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #105: GFLOPs: 94.4005. Time: 2185.6210 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #106: GFLOPs: 143.7430. Time: 1435.3654 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #107: GFLOPs: 139.2663. Time: 1481.5053 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #108: GFLOPs: 135.6136. Time: 1521.4091 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #109: GFLOPs: 139.1897. Time: 1482.3205 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #110: GFLOPs: 126.5679. Time: 1630.1422 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #111: GFLOPs: 151.9814. Time: 1357.5589 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #112: GFLOPs: 165.2776. Time: 1248.3462 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #113: GFLOPs: 151.4112. Time: 1362.6717 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #114: GFLOPs: 101.6183. Time: 2030.3786 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #115: GFLOPs: 94.1885. Time: 2190.5397 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #116: GFLOPs: 190.9152. Time: 1080.7085 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #117: GFLOPs: 100.9010. Time: 2044.8142 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #118: GFLOPs: 170.9793. Time: 1206.7178 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #119: GFLOPs: 150.8240. Time: 1367.9769 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #120: GFLOPs: 150.4303. Time: 1371.5567 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #121: GFLOPs: 131.9912. Time: 1563.1624 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #122: GFLOPs: 86.2131. Time: 2393.1833 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #123: GFLOPs: 182.7562. Time: 1128.9562 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #124: GFLOPs: 109.0544. Time: 1891.9342 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #125: GFLOPs: 114.7656. Time: 1797.7833 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #126: GFLOPs: 108.6228. Time: 1899.4510 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #127: GFLOPs: 28.6152. Time: 7210.2953 us. Best GFLOPs: 192.0279
2023-02-16 16:18:44 [INFO] [task_scheduler.cc:129] [Task #22: fused_nn_contrib_conv2d_NCHWc_add_nn_relu_4] Trial #128: GFLOPs: 27.8477. Time: 7409.0113 us. Best GFLOPs: 192.0279
